---
title: "Decision Trees and Random Forests on Forest Fires"
output: html_notebook
---
Used the resources provided, things that I learned previously, and [this website](https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d#:~:text=The%20most%20important%20hyper%2Dparameters,MSE%20or%20MAE%20for%20regression)

## Start
### Loading Libraries and Data
```{r}
# load packages
library(tidyverse) #for ggplot and dplyr
library(corrplot) #for correlation plots
# decision tree packages
library(rpart)
library(rpart.plot)
library(caTools)
library(RColorBrewer)
library(rattle)
# random forest package
library(randomForest)
```

```{r}
setwd("C:/Users/ezhou/OneDrive - Eastside Preparatory School/Documents/Eric/2022 - 2023/1 Fall/A Data Science/Forest Fires")
forestRaw <- read.csv("forestfires.csv")
```


### Data Preprocessing
```{r}
# summary(forestRaw)

forestNum <- forestRaw %>% select(-month, -day)
corrs <- cor(forestNum)
corrplot(corrs, order = 'hclust', addrect = 2)

# RMSE function
myRMSE <- function(pred, obs) {
  return(sqrt(mean((pred - obs) ^ 2, na.rm = TRUE)))
}
```

```{r}
forestNum <- forestNum %>% mutate(logArea = log(area+1))
# summary(forestNum)
```

## Decision Tree
Decision trees are flow-chart-like structures that are used to make decisions. Each node in the tree is like an if-statement, where those fulfilling the condition go down on branch and those that don't go down another. This keeps going until a leaf node, which is the final value/categorization you arrive at. So basically, based on the value of a variety of features that are evaluated in an order, you get a value/categorization. I believe the primary parameter to tune is the complexity parameter (cp), which determines how much of the tree to prune.

### Splitting Data
```{r}
set.seed(49301)
ind <- sample.split(Y=forestNum, SplitRatio=0.8)
trainData <- forestNum[ind,]
testData <- forestNum[-ind,]
```

### Creating Tree
```{r}
start1 <- Sys.time() # starting time
output.tree <- rpart(logArea ~.-area,data=trainData, method='anova')
end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # total time
```
Creating decision tree is a super fast process, only around 23 milliseconds.

### Visualizing the Tree
```{r}
# plot(output.tree)
# text(output.tree, pretty=0)
# summary(output.tree)
fancyRpartPlot(output.tree)
```


### Checking Results
```{r}
prediction_model <- predict(output.tree,testData,type="vector")
printcp(output.tree)
```
Here, we can see that as cp decreases, the error drops as well.

### Pruning Tree
```{r}
start1 <- Sys.time() # starting time
pruned_model <- prune.rpart(output.tree,cp=0.0125)
fancyRpartPlot(pruned_model)
y1 <- predict(pruned_model, testData)
pruned_model2 <- prune.rpart(output.tree,cp=0.02)
fancyRpartPlot(pruned_model2)
y2 <- predict(pruned_model2, testData)
end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # total time
```

### Testing Decision Trees
```{r}
myRMSE(prediction_model, testData$logArea)
myRMSE(y1, testData$logArea)
myRMSE(y2, testData$logArea)
```
As we can see, pruning here actually made the results worse. This makes sense because at the printcp() portion earlier, we can see that the lower the cp, the better the decision tree performed. It seems like the model does not have many unnecessary parts and simplifying the model only makes the results worse. I also tried dropping the cp lower than 0.01 (which is what the decision tree is at), but the tree and RMSE remains the same.
<br>
Overall, the performance is not bad but around the same as the linear model. It could definitely be improved further. 


## Random Forest
Random forests are a group of decision trees built from samples taken from the dataset. The prediction of the overall forest is the average of the predictions for each individual tree. I believe the main parameters we are able to tune are the number of trees that make up the forest and the number of random parameters to consider for each split.

### Creating random forest
```{r}
start1 <- Sys.time() # starting time
forest <- randomForest(formula=logArea ~ .-area, data=trainData)
end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # total time
forest
plot(forest)
varImpPlot(forest)
```

Although it's double the time decision tree took, it still was very fast and only took around 47 milliseconds to run. In the graphs, we can see that the error drops as the number of trees increases but it pretty much flattens out at a bit less than 50 trees. Also, Temperature is a very important predictor variable with many others closely grouped in the middle and rain being left way behind. There are 4 total factors above 80 for increase in node purity. Temperature and humidity being the top two makes a lot of sense to me but I was surprised that rain was so low on the list.

### Tune the Model
```{r}
start1 <- Sys.time() # starting time
moded_tuned <- tuneRF(
  x=trainData[,-2],
  y=trainData$logArea,
  ntreeTry=500,
  mtryStart = 4,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE
)
end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # total time
```
Since 500 trees seemed pretty good in the earlier graph, I chose to continue with it and test the number of parameters being tried. This took around 2.4 seconds to run, which is pretty slow. This makes sense though since we are making it train over and over to test the error varying the mtry. The drops in the graph make sense when looking at the variable importance from earlier. The increase in mtry increases the chance that the most important variable is included in some of the trees. At the very end, the final variables don't add too much as seen with the close correlation at the start.


### Testing Tuned Model
```{r}
start1 <- Sys.time() # starting time
tuned_forest <- randomForest(formula=logArea ~ .-area, data=trainData, mtry=9)
end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # total time

forest_pred <- predict(forest, testData)
tforest_pred <- predict(tuned_forest, testData)
myRMSE(forest_pred, testData$logArea)
myRMSE(tforest_pred, testData$logArea)
```

Time was a bit slower, around 1.3 seconds to train the new forest. Makes sense because there are more parameters to try splitting with. The tuned model is a bit better than the default model. Not sure if have 9 as mtry is too high and will cause repetition but it seems to work.

## Results
```{r}
myRMSE(prediction_model, testData$logArea)
myRMSE(y1, testData$logArea)
myRMSE(y2, testData$logArea)
myRMSE(forest_pred, testData$logArea)
myRMSE(tforest_pred, testData$logArea)
```

In the end, the not pruned version of the decision tree was the best decision tree but the random forests had way lower RMSE values. The tuned random forest was the best model overall out of the two models we have tried.